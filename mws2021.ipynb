{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fexrd-2021.2-py3-none-any.whl\n",
    "!wget https://github.com/FFRI/FEXRD/releases/download/v2021.3/fexrd-2021.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = trainデータへのパス\n",
    "test_path = testデータへのパス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fexrd import AllFeaturesExtractor\n",
    "fe = AllFeaturesExtractor(\"v2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path: str):\n",
    "    vecs = list()\n",
    "    row_names = list()\n",
    "    column_names = None\n",
    "    labels = list()\n",
    "    dfs = list()\n",
    "    with open(path, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            obj = json.loads(line)\n",
    "            column_names, vec = fe.get_features(\n",
    "                obj\n",
    "            )\n",
    "            label = obj[\"label\"]\n",
    "            vecs.append(vec)\n",
    "            row_names.append(obj[\"hashes\"][\"sha256\"])\n",
    "            labels.append(label)\n",
    "        df = pd.DataFrame(data=vecs, columns=column_names)\n",
    "        df.index = row_names\n",
    "        df[\"labels\"] = labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test(path: str):\n",
    "    vecs = list()\n",
    "    row_names = list()\n",
    "    column_names = None\n",
    "    labels = list()\n",
    "    dfs = list()\n",
    "    with open(path, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            obj = json.loads(line)\n",
    "            column_names, vec = fe.get_features(\n",
    "                obj\n",
    "            )\n",
    "            vecs.append(vec)\n",
    "        df = pd.DataFrame(data=vecs, columns=column_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#エントロピーに関する特徴量の追加\n",
    "def has_high_entropy_section(sections):\n",
    "    for section in sections:\n",
    "        if section[\"entropy\"] >= 7:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_low_entropy_section(sections):\n",
    "    for section in sections:\n",
    "        if section[\"entropy\"] <= 1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_extract_field(dat):\n",
    "    return {\n",
    "        \"sizeof_uninitialized_data\": dat[\"lief\"][\"optional_header\"][\"sizeof_uninitialized_data\"],\n",
    "        \"file_size\": dat[\"file_size\"],\n",
    "        \"characteristics\": dat[\"lief\"][\"header\"][\"characteristics\"],\n",
    "        \"has_debug\": \"debug\" in dat[\"lief\"].keys(),\n",
    "        \"has_coff_debug\": dat[\"lief\"][\"header\"][\"pointerto_symbol_table\"] != 0,\n",
    "        \"PEiD\": \", \".join(dat[\"peid\"][\"PEiD\"]),\n",
    "        \"impfuzzy\": dat[\"hashes\"][\"impfuzzy\"],\n",
    "        \"entropy_larger_than_7\": has_high_entropy_section(dat[\"lief\"][\"sections\"]),\n",
    "        \"entropy_smaller_than_1\": has_low_entropy_section(dat[\"lief\"][\"sections\"])\n",
    "    }\n",
    "\n",
    "def extract_field_for_train(line):\n",
    "    dat = json.loads(line)\n",
    "    label = {\"label\": dat[\"label\"]}\n",
    "    features = internal_extract_field(dat)\n",
    "    return dict(label, **features)\n",
    "\n",
    "def extract_field_for_test(line):\n",
    "    dat = json.loads(line)\n",
    "    return internal_extract_field(dat)\n",
    "\n",
    "# JSONLから必要なデータを読み取る関数\n",
    "def read_nested_json(path, extract_field):\n",
    "    with open(path, \"rb\") as f:\n",
    "        dat = pd.json_normalize([extract_field(l) for l in f.readlines()])\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量を追加する（trainデータ用）\n",
    "def make_additional_features_for_train(path):\n",
    "    df = read_nested_json(path, extract_field_for_train)\n",
    "    df[\"bss_larger_than_file_size\"] = (df[\"sizeof_uninitialized_data\"] > df[\"file_size\"]) * 1\n",
    "    df[\"has_debug_all\"] = (df[\"has_debug\"] | df[\"has_coff_debug\"]) * 1\n",
    "    IMAGE_FILE_RELOCS_STRIPPED = 1\n",
    "    IMAGE_FILE_LINE_NUMS_STRIPPED = 4\n",
    "    IMAGE_FILE_LOCAL_SYMS_STRIPPED = 8\n",
    "    df[\"entropy_larger_than_7\"] = df[\"entropy_larger_than_7\"] * 1\n",
    "    df[\"entropy_smaller_than_1\"] = df[\"entropy_smaller_than_1\"] * 1\n",
    "    df[\"relocs_stripped\"] = ((df[\"characteristics\"] & IMAGE_FILE_RELOCS_STRIPPED) != 0) * 1\n",
    "    df[\"line_nums_stripped\"] = ((df[\"characteristics\"] & IMAGE_FILE_LINE_NUMS_STRIPPED) != 0) * 1\n",
    "    df[\"local_syms_stripped\"] = ((df[\"characteristics\"] & IMAGE_FILE_LOCAL_SYMS_STRIPPED) != 0) * 1\n",
    "    df[\"includes_upx\"] = df[\"PEiD\"].map(lambda x: \"UPX\" in x if x else False) * 1\n",
    "    df[\"includes_ASPack\"] = df[\"PEiD\"].map(lambda x: \"ASPack\" in x if x else False) * 1\n",
    "    df[\"inclues_Obsidium\"] = df[\"PEiD\"].map(lambda x: \"Obsidium\" in x if x else False) * 1\n",
    "    df = df.drop([\"label\", \"PEiD\", \"impfuzzy\", \"has_debug\", \"has_coff_debug\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_additional_features_for_test(path):\n",
    "    df = read_nested_json(path, extract_field_for_test)\n",
    "    df[\"bss_larger_than_file_size\"] = (df[\"sizeof_uninitialized_data\"] > df[\"file_size\"]) * 1\n",
    "    df[\"has_debug_all\"] = (df[\"has_debug\"] | df[\"has_coff_debug\"]) * 1\n",
    "    IMAGE_FILE_RELOCS_STRIPPED = 1\n",
    "    IMAGE_FILE_LINE_NUMS_STRIPPED = 4\n",
    "    IMAGE_FILE_LOCAL_SYMS_STRIPPED = 8\n",
    "    df[\"entropy_larger_than_7\"] = df[\"entropy_larger_than_7\"] * 1\n",
    "    df[\"entropy_smaller_than_1\"] = df[\"entropy_smaller_than_1\"] * 1\n",
    "    df[\"relocs_stripped\"] = ((df[\"characteristics\"] & IMAGE_FILE_RELOCS_STRIPPED) != 0) * 1\n",
    "    df[\"line_nums_stripped\"] = ((df[\"characteristics\"] & IMAGE_FILE_LINE_NUMS_STRIPPED) != 0) * 1\n",
    "    df[\"local_syms_stripped\"] = ((df[\"characteristics\"] & IMAGE_FILE_LOCAL_SYMS_STRIPPED) != 0) * 1\n",
    "    df[\"includes_upx\"] = df[\"PEiD\"].map(lambda x: \"UPX\" in x if x else False) * 1\n",
    "    df[\"includes_ASPack\"] = df[\"PEiD\"].map(lambda x: \"ASPack\" in x if x else False) * 1\n",
    "    df[\"inclues_Obsidium\"] = df[\"PEiD\"].map(lambda x: \"Obsidium\" in x if x else False) * 1\n",
    "    df = df.drop([\"PEiD\", \"impfuzzy\", \"has_debug\", \"has_coff_debug\"], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fexrd_df = extract(train_path)\n",
    "train_df=train_fexrd_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_train_features = make_additional_features_for_train(train_path)\n",
    "additional_test_features = make_additional_features_for_test(test_path)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_full = pd.concat([train_df, additional_train_features], axis=1)\n",
    "test_features_full = pd.concat([test_df, additional_test_features], axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =  train_features_full['labels'].to_numpy()\n",
    "X = train_features_full.drop('labels', axis=1).fillna(0).to_numpy()\n",
    "test_features_full.drop('manalyze_plugin_packer_manalyze_output.ManalyzeDetectionReason.UNUSUAL_SECTION_NAME', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf_ex = ExtraTreesClassifier(n_estimators=400,random_state=42)\n",
    "clf_ex.fit(X, y)\n",
    "pred_ex = np.where(clf_ex.predict_proba(test_features_full)[:, 1] < 0.90, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ran = RandomForestClassifier(n_estimators=400)\n",
    "clf_ran.fit(X, y)\n",
    "a=clf_ran.predict(test_features_full)\n",
    "pred_ran = np.where(a< 0.9, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 8, 16)\n",
    "    max_leaf_nodes = int(trial.suggest_discrete_uniform(\"max_leaf_nodes\", 4, 64, 4))\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "    \n",
    "    RFC = RandomForestClassifier(min_samples_split = min_samples_split, \n",
    "                                max_leaf_nodes = max_leaf_nodes,\n",
    "                                criterion = criterion)\n",
    "    RFC.fit(X, y)\n",
    "    return 1.0 - accuracy_score(y_test, RFC.predict(X_test))\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=test_features_full.index.tolist()\n",
    "your_first_answer = pd.DataFrame({\"id\": b, \"label\": pred_ex})\n",
    "your_first_answer.to_csv(\"/content/drive/MyDrive/mws_for_spring/sub.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
